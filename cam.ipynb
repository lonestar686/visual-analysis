{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Mapping In PyTorch\n",
    "Have you ever wondered just how a neural network model like ResNet decides on its decision to determine that an image is a cat or a flower in the field? Class Activation Mappings (CAM) can provide some insight into this process by overlaying a heatmap over the original image to show us where our model thought most strongly that this cat was indeed a cat.\n",
    "\n",
    "This script will demonstrate how to use a pretrained model, in PyTorch, \n",
    "to make predictions. Specifically, we will be using VGG16 with a cat \n",
    "image.\n",
    "\n",
    "References used to make this script:\n",
    "\n",
    "PyTorch pretrained models doc:\n",
    "\n",
    "    http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "PyTorch image transforms example:\n",
    "\n",
    "    http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
    "\n",
    "Example code:\n",
    "\n",
    "    http://blog.outcome.io/pytorch-quick-start-classifying-an-image/\n",
    "\n",
    "Firstly, we’re going to need a picture of a cat. And thankfully, here’s one I took earlier of a rather suspicious cat that is wondering why the strange man is back in his house again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.transform\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torch.nn import functional as F\n",
    "from torch import topk\n",
    "\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image\n",
    "IMG_URL = 'http://media.mlive.com/news_impact/photo/9933031-large.jpg'\n",
    "\n",
    "# Random cat img taken from Google\n",
    "# IMG_URL = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg'\n",
    "\n",
    "# Class labels used when training VGG as json, courtesy of the 'Example code' link above.\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our class labels.\n",
    "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
    "labels = {int(key): value for key, value in response.json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our img.\n",
    "response = requests.get(IMG_URL)\n",
    "# Read bytes and store as an img.\n",
    "image = Image.open(io.BytesIO(response.content))\n",
    "\n",
    "# let's take a look at it\n",
    "imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’re going to set up some torchvision transforms to scale the image to the 224x224 required for ResNet and also to normalize it to the ImageNet mean/std. \n",
    "\n",
    "Now that we have an img, we need to preprocess it.\n",
    "We need to:\n",
    "* resize the img, it is pretty big (~1200x1200px).\n",
    "* normalize it, as noted in the PyTorch pretrained models doc, with, mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
    "* convert it to a PyTorch Tensor.\n",
    "\n",
    "We can do all this preprocessing using a transform pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet mean/std\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Preprocessing - scale to 224x224 for model, convert to tensor, \n",
    "# and normalize to -1..1 with mean/std for ImageNet\n",
    "min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((min_img_size,min_img_size)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "display_transform = transforms.Compose([\n",
    "   transforms.Resize((224,224))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = preprocess(image)\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
    "# Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
    "img = img_tensor.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n",
    "\n",
    "# send it to gpu and track the graph\n",
    "prediction_var = img.cuda()\n",
    "prediction_var.requires_grad_();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having converted our image into a PyTorch variable, we need a model to generate a prediction. Let’s use ResNet18, put it in evaluation mode, and stick it on the GPU using the CUDA libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's load our model\n",
    "model = models.resnet18(pretrained=True);\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit of code is swiped from Jeremy Howard’s fast.ai course. It basically allows you to easily attach a hook to any model (or any part of a model - here we’re going to grab the final convnet layer in ResNet18) which will save the activation features as an instance variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = ((output.cpu()).data).numpy()\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = model._modules.get('layer4')\n",
    "\n",
    "activated_features = SaveFeatures(final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set that up, we run the image through our model and get the prediction. We then run that through a softmax layer to turn that prediction into a series of probabilities for each of the 1000 classes in ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and get a prediciton!\n",
    "prediction = model(prediction_var)\n",
    "# Returns a Tensor of shape (batch, num class labels)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probabilities = F.softmax(prediction, dim=-1).data.squeeze()\n",
    "activated_features.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using topk(), we can see that our model is 36% confident that this picture is class 282. Looking that up in the ImageNet classes, that gives us…’tiger cat’. I would say that’s not a bad guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, class_idx = topk(pred_probabilities,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (prob.item(), class_idx.item(), labels[class_idx.item()])  # Converts the index to a string using our labels dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having made the guess, let’s see where the neural network was focussing its attention. The getCAM() method here takes the activated features of the convnet, the weights of the fully-connected layer (on the side of the average pooling), and the class index we want to investigate (282/‘tiger cat’ in our case). We index into the fully-connected layer to get the weights for that class and calculate the dot product with our features from the image.\n",
    "\n",
    "(this code is based on the paper that introduced CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCAM(feature_conv, weight_fc, class_idx):\n",
    "    _, nc, h, w = feature_conv.shape\n",
    "    cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    return [cam_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bias = model._modules.get('fc').parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = tuple(weight_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_softmax = weight.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weight_softmax[class_idx.item()]\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = getCAM(activated_features.features, weight_softmax, class_idx.item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see our heatmap and overlay it onto Casper. It doesn’t make him look any happier, but we can see exactly where the model made its mind up about him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(overlay[0], alpha=0.5, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(skimage.transform.resize(overlay[0], img_tensor.shape[1:3]), cmap='jet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(display_transform(image))\n",
    "imshow(skimage.transform.resize(overlay[0], img_tensor.shape[1:3]), alpha=0.5, cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, there’s a bit more - we can also look at the model’s second choice for cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, class_indices = topk(pred_probabilities,2)\n",
    "probs, class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs[0].item(), class_indices[0].item(), labels[class_indices[0].item()])\n",
    "print(probs[1].item(), class_indices[1].item(), labels[class_indices[1].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = getCAM(activated_features.features, weight_softmax, class_indices[1].item() )\n",
    "imshow(skimage.transform.resize(overlay[0], img_tensor.shape[1:3]), alpha=0.5, cmap='jet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(display_transform(image))\n",
    "imshow(skimage.transform.resize(overlay[0], img_tensor.shape[1:3]), alpha=0.5, cmap='jet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
