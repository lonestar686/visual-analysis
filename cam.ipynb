{
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Class Activation Mapping In PyTorch\n",
"Have you ever wondered just how a neural network model like ResNet decides on its decision to determine that an image is a cat or a flower in the field? Class Activation Mappings (CAM) can provide some insight into this process by overlaying a heatmap over the original image to show us where our model thought most strongly that this cat was indeed a cat.\n",
"\n",
"This script will demonstrate how to use a pretrained model, in PyTorch,\n",
"\n",
"To make predictions. Specifically, we will be using VGG16 with a cat image.\n",
"\n",
"References used to make this script:\n",
"\n",
"PyTorch pretrained models doc:\n",
"\n",
"    http://pytorch.org/docs/master/torchvision/models.html\n",
"\n",
"PyTorch image transforms example:\n",
"\n",
"    http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms\n",
"\n",
"Example code:\n",
"\n",
"    http://blog.outcome.io/pytorch-quick-start-classifying-an-image/\n",
"\n",
"Firstly, we’re going to need a picture of a cat. And thankfully, here’s one I took earlier of a rather suspicious cat that is wondering why the strange man is back in his house again."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"%matplotlib inline\n",
"%reload_ext autoreload\n",
"%autoreload 2"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"import numpy as np\n",
"import skimage.transform\n",
"from PIL import Image\n",
"from matplotlib.pyplot import imshow\n",
"\n",
"from torchvision import models, transforms\n",
"from torch.nn import functional as F\n",
"from torch import topk\n",
"\n",
"import io\n",
"import requests"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# ImageNet class-index-label\n",
"import json\n",
"\n",
"# on cloud\n",
"# LABELS_URL = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
"\n",
"# at local disk\n",
"LABELS_URL = \"./imagenet_class_index.json\"\n",
"\n",
"# {\"0\": [\"n01440764\", \"tench\"], \"1\": [\"n01443537\", \"goldfish\"], \"2\": [\"n01484850\", \"great_white_shark\"], ...}\n",
"\n",
"idx2label = []\n",
"cls2label = {}\n",
"# load imagenet class-index\n",
"with open(LABELS_URL, \"r\") as read_file:\n",
"    class_indices = json.load(read_file)\n",
"    # convert index or class name to label\n",
"    idx2label = [class_indices[str(k)][1] for k in range(len(class_indices))]\n",
"    cls2label = {class_indices[str(k)][0]: class_indices[str(k)][1] for k in range(len(class_indices))}"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# input image\n",
"IMG_URL = 'http://media.mlive.com/news_impact/photo/9933031-large.jpg'\n",
"\n",
"# IMG_URL = 'https://cdn.pixabay.com/photo/2017/02/20/18/03/cat-2083492__340.jpg'\n",
"# IMG_URL = 'https://cdn.pixabay.com/photo/2018/03/26/20/49/tiger-3264048__340.jpg'\n",
"# IMG_URL = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSyOX2T4HLkL_PUaMgKoo6q_NTk0thueXbRN3OAI-2fRyRz4E0iEg'"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"response = requests.get(IMG_URL)\n",
"image = Image.open(io.BytesIO(response.content))\n",
"np.array(image).shape"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"imshow(image)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Next, we’re going to set up some torchvision transforms to scale the image to the 224x224 required for ResNet and also to normalize it to the ImageNet mean/std.\n",
"\n",
"Now that we have an img, we need to preprocess it. We need to:\n",
"\n",
"- Resize the image to its minimal size, required by Resnet.\n",
"- Convert it to a PyTorch Tensor.\n",
"- Normalize it, as noted in the PyTorch pretrained models doc, with mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n",
"\n",
"We can do all this preprocessing using a transform pipeline."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Imagenet mean/std\n",
"normalize = transforms.Normalize(\n",
"   mean=[0.485, 0.456, 0.406],\n",
"   std=[0.229, 0.224, 0.225]\n",
")\n",
"\n",
"# Preprocessing - scale to 224x224 for model, convert to tensor, \n",
"# and normalize to -1..1 with mean/std for ImageNet\n",
"min_img_size = 224  # The min size, as noted in the PyTorch pretrained models doc, is 224 px.\n",
"preprocess = transforms.Compose([\n",
"   transforms.Resize((min_img_size, min_img_size)),\n",
"   transforms.ToTensor(),\n",
"   normalize\n",
"])\n",
"\n",
"display_transform = transforms.Compose([\n",
"   transforms.Resize((224,224))])"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"img_tensor = preprocess(image)\n",
"print(img_tensor.shape)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
"# Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
"img_var = img_tensor.unsqueeze(0) # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n",
"\n",
"# send it to gpu and track the graph\n",
"img_var = img_var.cuda()\n",
"img_var.requires_grad_();"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Having converted our image into a PyTorch variable, we need a model to generate a prediction. Let’s use ResNet18, put it in evaluation mode, and stick it on the GPU using the CUDA libraries."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"model = models.resnet18(pretrained=True)\n",
"model.cuda()\n",
"model.eval()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"This next bit of code is swiped from Jeremy Howard’s fast.ai course. It basically allows you to easily attach a hook to any model (or any part of a model - here we’re going to grab the final convnet layer in ResNet18) which will save the activation features as an instance variable."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"class SaveFeatures():\n",
"    features=None\n",
"    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
"    def hook_fn(self, module, inp, outp): self.features = ((outp.cpu()).data).numpy()\n",
"    def remove(self): self.hook.remove()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"final_layer = model._modules.get('layer4')\n",
"\n",
"activated_features = SaveFeatures(final_layer)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Having set that up, we run the image through our model and get the prediction. We then run that through a softmax layer to turn that prediction into a series of probabilities for each of the 1000 classes in ImageNet."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"prediction = model(img_var)\n",
"pred_probabilities = F.softmax(prediction).data.squeeze()\n",
"activated_features.remove()"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Using topk(), we can see that our model is 78% confident that this picture is class 283. Looking that up in the ImageNet classes, that gives us…’persian cat’. I would say that’s not a bad guess!"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"probs, indices = topk(pred_probabilities,1)\n",
"probs, indices"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Having made the guess, let’s see where the neural network was focussing its attention. The getCAM() method here takes the activated features of the convnet, the weights of the fully-connected layer (on the side of the average pooling), and the class index we want to investigate (283/‘persian cat’ in our case). We index into the fully-connected layer to get the weights for that class and calculate the dot product with our features from the image.\n",
"\n",
"(this code is based on the paper that introduced CAM)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def getCAM(feature_conv, weight_fc, class_idx):\n",
"    _, nc, h, w = feature_conv.shape\n",
"    cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
"    cam = cam.reshape(h, w)\n",
"    cam = cam - np.min(cam)\n",
"    cam_img = cam / np.max(cam)\n",
"    return cam_img\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"weight, bias = tuple(model._modules.get('fc').parameters())\n",
"weight_softmax = weight.cpu().data.numpy()\n",
"print(weight_softmax.shape)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"top_index = 0\n",
"\n",
"# convert to cpu\n",
"prob = probs[top_index].item() * 100\n",
"class_idx = indices[top_index].item()\n",
"\n",
"# get CAM\n",
"overlay = getCAM(activated_features.features, weight_softmax, class_idx )"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"#\n",
"prob, class_idx, idx2label[class_idx]"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Now we can see our heatmap and overlay it onto Casper. It doesn’t make him look any happier, but we can see exactly where the model made its mind up about him."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"imshow(overlay, cmap='jet')"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"imshow(skimage.transform.resize(overlay, img_tensor.shape[1:3]), cmap='jet');"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"imshow(display_transform(image))\n",
"imshow(skimage.transform.resize(overlay, img_tensor.shape[1:3]), alpha=0.5, cmap='jet');"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"But wait, there’s a bit more - we can also look at the model’s second choice for Casper."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"probs, indices = topk(pred_probabilities,2)\n",
"probs, indices"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"top_index = 1\n",
"\n",
"# convert to cpu\n",
"prob = probs[top_index].item() * 100\n",
"class_idx = indices[top_index].item()\n",
"\n",
"# get CAM\n",
"overlay = getCAM(activated_features.features, weight_softmax, class_idx )"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"#\n",
"prob, class_idx, idx2label[class_idx]"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"imshow(display_transform(image))\n",
"imshow(skimage.transform.resize(overlay, img_tensor.shape[1:3]), alpha=0.5, cmap='jet');"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"Although the heatmap is similar, the network is focussing a touch more on his fluffy coat to suggest he might be class 332 - an Angora rabbit. And well, he is a Turkish Angora cat after all…"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": []
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.6.5"
}
},
"nbformat": 4,
"nbformat_minor": 2
}